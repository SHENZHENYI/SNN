{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of temproal_NMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXwYf8daTHv0",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQR4_QtoTKhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "with zipfile.ZipFile('NMNISTsmall.zip') as zip_file:\n",
        "    for member in zip_file.namelist():\n",
        "        if not os.path.exists('./' + member):\n",
        "            zip_file.extract(member, './')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgT6e1ULTAGC",
        "colab_type": "code",
        "outputId": "71d0d80c-9013-4833-dccf-fc02d3960973",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import numpy as np\n",
        "import spikeFileIO as io\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noCmUW3kTGho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#lets collect addresses of training and test data\n",
        "def load(fname):\n",
        "    ''' Load the file using std open'''\n",
        "    f = open(fname,'r')\n",
        "\n",
        "    data = []\n",
        "    for line in f.readlines():\n",
        "        data.append(line.replace('\\n','').split(' '))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "path = 'NMNISTsmall/'\n",
        "#train data\n",
        "training_files = load('NMNISTsmall/train1K.txt')\n",
        "training_files = training_files[1:]\n",
        "\n",
        "training_addrs = []\n",
        "training_labels = []\n",
        "for i in range (len(training_files)):\n",
        "  training_addrs.append(path+training_files[i][0][:-2]+'.bs2')\n",
        "  training_labels.append(training_files[i][0][-1])\n",
        "\n",
        "#test data\n",
        "test_files = load('NMNISTsmall/test100.txt')\n",
        "test_files = test_files[1:]\n",
        "\n",
        "test_addrs = []\n",
        "test_labels = []\n",
        "for i in range (len(test_files)):\n",
        "  test_addrs.append(path+test_files[i][0][:-2]+'.bs2')\n",
        "  test_labels.append(test_files[i][0][-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1EZ_3_oVVO7",
        "colab_type": "text"
      },
      "source": [
        "Data representation review:\n",
        "\n",
        "In the original paper, the spiking MNIST set is made according to the raw pixel brightness values. All spiking time is in a range from 0 to 1. They code dark pixels eariler than brighter ones. Those pixels not spiking are recognised as not-spiking-neurons which is dealt specifically in the training process.\n",
        "\n",
        "Now the problem becomes that how can we construct the data structure of NMNIST, and still do the work.\n",
        "\n",
        "The first difference between the Google devised data and our data is that their input neuron can have 1 or 0 spike at a specific pixel, wheras our data can have multiple spikes in one pixel. Take the 0th sample as an example, it has 4681 spikes, while 4197 of pixels spike more than once.\n",
        "\n",
        "\n",
        "Now my decision is to only retain the first spiking time of each pixel following the logic of the original paper: they only retain the first spike in the hidden units. We will see what we get. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFoK64m9VT5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xyp21dim_onlyfirstspikepreserved(TD):\n",
        "  TD_temporal = torch.ones((34,34,2))*2.8 #assumed that 2.8 is a mask number, identified as no-spiking time. 2.8 is due to historical reason\n",
        "  for i in range(TD.x.shape[0]):\n",
        "    if TD_temporal[TD.x[i], TD.y[i], TD.p[i]] == 2.8:\n",
        "      TD_temporal[TD.x[i], TD.y[i], TD.p[i]] = TD.t[i]/150 #normalisation; taking 150 as the expected largest time \n",
        "  return TD_temporal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Of53-gTOU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_data_generator(X, y, batch_size, shuffle=True, device = 'cuda', dtype=torch.float):\n",
        "    \"\"\" This generator takes training data's address and generates spiking network input as dense tensors. \n",
        "\n",
        "    Args:\n",
        "        X: The data ( 'data/userx_lighting_conditions/y.npy' )\n",
        "        y: The labels\n",
        "        batch_size: batch size\n",
        "    \"\"\" \n",
        "    labels_ = np.array(y,dtype=np.int)\n",
        "    number_of_batches = len(X)//batch_size\n",
        "    sample_index = np.arange(len(X))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "\n",
        "    counter = 0\n",
        "    while counter<number_of_batches:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        X_batch = torch.empty((batch_size,2*34*34),dtype=dtype)\n",
        "        for bc,i in enumerate(batch_index):\n",
        "            TD = io.read2Dspikes(X[i])\n",
        "            X_batch_temp = xyp21dim_onlyfirstspikepreserved(TD)\n",
        "            #flatten dimensions and save in X_batch\n",
        "            X_batch[bc] = X_batch_temp.view((-1))\n",
        "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
        "        yield [X_batch.to(device=device), y_batch.to(device=device)] \n",
        "        counter+=1    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5nTmcjaqFPJ",
        "colab_type": "text"
      },
      "source": [
        "# Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCZfWyT_bN_6",
        "colab_type": "text"
      },
      "source": [
        "Kernel generates spiking time of a neuron given some input spike time "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iurUZwg5i-fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alpha_kernel_response_temporal_v2(input_time, weights, tau = 1, thre = 1, t_max = 300): #faster implementation\n",
        "  #argument: input_time shape = (n_batch x n_input);  weights shape = (n_input x n_output)\n",
        "  #alpha_kernel_response is t*e^(tau*t); the increment of this kernel is (1+t*tau)*exp(tau*t)\n",
        "  n_batch = input_time.shape[0]\n",
        "  n_input = input_time.shape[1]\n",
        "  n_output = weights.shape[1]\n",
        "  precision = 100.0\n",
        "\n",
        "  v_accu = torch.zeros((n_batch, n_output), device = input_time.device)\n",
        "  spike_time = torch.zeros_like(v_accu)\n",
        "  spike_time_mem = torch.ones_like(v_accu)\n",
        "  for t in range(t_max):\n",
        "    t_clamp = torch.clamp(t/precision -input_time, 0, t_max)\n",
        "    invalid_incr = (t_clamp==0)\n",
        "    v_incr = torch.exp(-tau*t_clamp)*(1-tau*t_clamp) #unweighted dv/dt\n",
        "    v_incr[invalid_incr] = 0\n",
        "    v_incr = torch.mm(v_incr, weights)/precision #dv/dt * delta_t\n",
        "    v_accu += v_incr #update the membrane potential at time instance t\n",
        "    index = (v_accu>thre) & (spike_time_mem == 1) #output index to update the earilest spiking time\n",
        "    spike_time[index] = t/precision\n",
        "    spike_time_mem[index] = 0 #mark the updated output index such that it will not be updated again\n",
        "  \n",
        "  index_nospike = (spike_time == 0)\n",
        "  spike_time[index_nospike] = 2.8 #an arbirary number that identifies no-spiking neuron\n",
        "  \n",
        "  return spike_time.to(input_time.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_StECwi5qXf0",
        "colab_type": "text"
      },
      "source": [
        "# Network Configuratoin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Q514pfXcnS",
        "colab_type": "text"
      },
      "source": [
        "Here we configure a SNN with input layer of a size 2x34x34, a hidden layer of a size 100, and an output layer of a size 10. Moreover, both the input layer and the hidden layer have one bias to prevent neuron from not spiking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpOS_flpqSN3",
        "colab_type": "code",
        "outputId": "4ecef25d-25a3-468c-fd65-8558b2acd841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nb_inputs  = 2*34*34\n",
        "nb_hidden  = 100\n",
        "nb_outputs = 10\n",
        "#we assume there is only 1 batch now\n",
        "\n",
        "#weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "dtype=torch.float\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#initialisation\n",
        "weight_scale = np.sqrt(2)\n",
        "\n",
        "nb_bias1 = 1\n",
        "nb_bias2 = 1\n",
        "\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=7.83912*weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias1), std=weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias1)) #the mean is brought up to 1.5 to force neurons to spike initially\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=7.83912*weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias2), std=weight_scale/np.sqrt(nb_hidden+nb_outputs+nb_bias2))\n",
        "\n",
        "bias1_time = torch.empty((nb_bias1),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.uniform_(bias1_time, a=0.0, b=1.0)\n",
        "bias1_weight = torch.empty((nb_bias1, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(bias1_weight, mean=-0.275419*weight_scale/np.sqrt(nb_inputs+nb_bias1+nb_hidden), std=weight_scale/np.sqrt(nb_inputs+nb_bias1+nb_hidden))\n",
        "#torch.nn.init.uniform_(bias1_weight, a=1.0, b=1.0)\n",
        "\n",
        "\n",
        "bias2_time = torch.empty((nb_bias2),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.uniform_(bias2_time, a=0.0, b=1.0)\n",
        "bias2_weight = torch.empty((nb_bias2, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(bias2_weight, mean=-0.275419*weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden), std=weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden))\n",
        "#torch.nn.init.uniform_(bias2_weight, a=1.0, b=1.0)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0367,  0.0213, -0.1721,  0.0224, -0.1343, -0.1208,  0.0648,  0.1651,\n",
              "         -0.1455, -0.1769]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U__OAbEBv15-",
        "colab_type": "code",
        "outputId": "fde2bffa-ad9b-4585-b7e5-29c8017acce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(bias1_time.shape)\n",
        "print(w1.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1])\n",
            "torch.Size([2312, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbZVG4RZqpht",
        "colab_type": "text"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feUAfoCdX6eb",
        "colab_type": "text"
      },
      "source": [
        "helper function that solves W-Lambert function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N5rceeTKZJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/google/ihmehimmeli/blob/master/tempcoding/lambertw.cc\n",
        "def LambertW0InitialGuess_vec(x):\n",
        "  kNearBranchCutoff = -0.3235\n",
        "  kE = 2.718281828459045\n",
        "  x_copy = x.clone()\n",
        "\n",
        "  # Sqrt approximation near branch cutoff.\n",
        "  index1 = (x < kNearBranchCutoff)\n",
        "  x_copy[index1] = -1.0+torch.sqrt(2.0*(1+kE*x[index1]))\n",
        "\n",
        "  # Taylor series between [-1/e and 1/e].\n",
        "  index2 = (x > kNearBranchCutoff) & (x < -kNearBranchCutoff)\n",
        "  x_copy[index2] = x[index2] * (1 + x[index2] * (-1 + x[index2] * (3.0 / 2.0 - 8.0 / 3.0 * x[index2])))\n",
        "\n",
        "  #Series of piecewise linear approximation\n",
        "  index3 = (x > -kNearBranchCutoff) & (x < 0.6)\n",
        "  x_copy[index3] =  0.23675531078855933 + (x[index3] - 0.3) * 0.5493610866617109\n",
        "  \n",
        "  index4 = (x > 0.6) & (x < 0.8999999999999999)\n",
        "  x_copy[index4] = 0.4015636367870726 + (x[index4] - 0.6) * 0.4275644294878729\n",
        "\n",
        "  index5 = (x > 0.8999999999999999) & (x < 1.2)\n",
        "  x_copy[index5] = 0.5298329656334344 + (x[index5] - 0.8999999999999999) * 0.3524368357714513\n",
        "\n",
        "  index6 = (x > 1.2) & (x < 1.5)\n",
        "  x_copy[index6] = 0.6355640163648698 + (x[index6] - 1.2) * 0.30099113800452154\n",
        "\n",
        "  index7 = (x > 1.5) & (x < 1.8)\n",
        "  x_copy[index7] = 0.7258613577662263 + (x[index7] - 1.5) * 0.2633490154764343\n",
        "  \n",
        "  index8 = (x > 1.8) & (x < 2.0999999999999996)\n",
        "  x_copy[index8] = 0.8048660624091566 + (x[index8] - 1.8) * 0.2345089875713013;\n",
        "  \n",
        "  index9 = (x > 2.0999999999999996) & (x < 2.4)\n",
        "  x_copy[index9] =  0.8752187586805469 + (x[index9] - 2.0999999999999996) * 0.2116494532726034\n",
        "\n",
        "  index10 = (x > 2.4) & (x < 2.6999999999999997)\n",
        "  x_copy[index10] = 0.938713594662328 + (x[index10] - 2.4) * 0.19305046534383152\n",
        "\n",
        "  index11 = (x > 2.6999999999999997) & (x < 2.9999999999999996)\n",
        "  x_copy[index11] = 0.9966287342654774 + (x[index11] - 2.6999999999999997) * 0.17760053566187495\n",
        "  \n",
        "  #asymptotic approxiamtion\n",
        "  index12 = ~(index1 + index2 + index3 + index4 + index5 + index6 +index7 +index8 +index9 +index10 + index11)\n",
        "  l = torch.log(x[index12])\n",
        "  ll = torch.log(l)\n",
        "  x_copy[index12] = l - ll + ll/l\n",
        "  return x_copy\n",
        "\n",
        "def LambertW0_vec(x):\n",
        "  x_copy = x.clone()\n",
        "  kReciprocalE = 0.36787944117\n",
        "  kDesiredAbsoluteDifference = 1e-3\n",
        "  kNumMaxIters = 10\n",
        "\n",
        "  index1 =  (x < -kReciprocalE)\n",
        "  x_copy[index1] = 0\n",
        "  #return x, False\n",
        "\n",
        "  index2 = (x == 0.0)\n",
        "  x_copy[index2] = 0\n",
        "\n",
        "  index3 = (x == -kReciprocalE)\n",
        "  x_copy[index3] = -1 \n",
        "\n",
        "  index4 = ~(index1 + index2 + index3)\n",
        "\n",
        "  #current guess\n",
        "  w_n = LambertW0InitialGuess_vec(x[index4])\n",
        "  have_convergence = False\n",
        "\n",
        "  #fritsch iteration\n",
        "  for i in range(1):\n",
        "    z_n = torch.log(x[index4] / w_n) - w_n\n",
        "    q_n = 2.0 * (1.0 + w_n) * (1.0 + w_n + 2.0 / 3.0 * z_n)\n",
        "    e_n = (z_n / (1.0 + w_n)) * ((q_n - z_n) / (q_n - 2.0 * z_n))\n",
        "    w_n = w_n * (1.0 + e_n)\n",
        "    #Done this way as the log is the expensive part above.\n",
        "    #if (torch.abs(z_n) < kDesiredAbsoluteDifference):\n",
        "\n",
        "  x_copy[index4] = w_n\n",
        "  return x_copy\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsrbscrmYSLk",
        "colab_type": "text"
      },
      "source": [
        "# Forward and backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBeu4hrGKU6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNN_process_v3(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias_time, bias_weight):\n",
        "        \"\"\"\n",
        "        Argument: input is input spike time of dimension (n_output)\n",
        "                  weight is of dimension (n_input, n_output)\n",
        "                  bias_time is of dimension (n_bias)\n",
        "                  bias_weight is of dimension (n_bias, n_output)\n",
        "        \"\"\"\n",
        "        #####firstly, we combine the bias and inputs, and their weights#####\n",
        "        batch_size = input.shape[0]\n",
        "        n_bias = bias_time.shape[0]\n",
        "        bias_time_broadcasted = bias_time.view(1,n_bias).repeat(batch_size,1)\n",
        "        x_n_bias = torch.cat((input, bias_time_broadcasted),-1)\n",
        "\n",
        "        w_n_bias = torch.cat((weight,bias_weight),0)\n",
        "\n",
        "        #####membrane dynamics begin here###################################\n",
        "        max_time_units = 900\n",
        "        tau = 2 #0.181769\n",
        "        theta = 0.5 #1.16732\n",
        "        not_spike_time = 2.8 #if the neuron does not spike, the spike time is assigned to 2.8 sec. It is a potential problem\n",
        "        nb_outputs = w_n_bias.shape[1]\n",
        "\n",
        "        spk = alpha_kernel_response_temporal_v2(x_n_bias, w_n_bias, tau = tau, thre = theta, t_max = max_time_units)\n",
        "        ctx.intermediate_results = x_n_bias, w_n_bias, spk, n_bias\n",
        "\n",
        "        return spk\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        x_n_bias, w1_n_bias, spk, n_bias = ctx.intermediate_results\n",
        "        tau = 2 #0.181769\n",
        "        theta = 0.5 #1.16732\n",
        "        input = x_n_bias\n",
        "        batch_size = x_n_bias.shape[0]\n",
        "        nb_inputs = w1_n_bias.shape[0]\n",
        "        nb_outputs = w1_n_bias.shape[1]\n",
        "\n",
        "        #t0 = time.time()\n",
        "        #compute ingredients of gradients \n",
        "        AI = torch.zeros_like(spk).to(input.device)  #initialisation\n",
        "        BI = torch.zeros_like(spk).to(input.device)\n",
        "        t_out = torch.zeros_like(spk).to(input.device)\n",
        "        WI = torch.zeros_like(spk).to(input.device)\n",
        "        grad_input = torch.zeros((batch_size, nb_inputs, nb_outputs)).to(input.device)\n",
        "        grad_weight = torch.zeros((batch_size, nb_inputs, nb_outputs)).to(input.device)\n",
        "\n",
        "        #extend matrices to a common shape\n",
        "        x_n_bias_modified = x_n_bias.unsqueeze(2).repeat(1,1,nb_outputs)\n",
        "        grad_output_modified = grad_output.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        w1_n_bias_repeated = w1_n_bias.unsqueeze(0).repeat(batch_size,1,1)\n",
        "\n",
        "        #find the index for valid inputs \n",
        "        spk_modified = spk.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        valid_index = x_n_bias_modified < spk_modified\n",
        "\n",
        "        #print(valid_index)\n",
        "        AI = torch.sum(torch.exp(x_n_bias_modified*tau)*w1_n_bias_repeated*valid_index, 1).to(x_n_bias.device)\n",
        "        BI = torch.sum(torch.exp(x_n_bias_modified*tau)*x_n_bias_modified*w1_n_bias_repeated*valid_index, 1).to(x_n_bias.device)\n",
        "        #t1 = time.time()\n",
        "        #print('AI and BI', t1-t0)\n",
        "        exploding_WI_idx_batch = []\n",
        "        exploding_WI_idx_out = []\n",
        "\n",
        "        WI_intermediate = -tau*theta/AI * torch.exp(tau*BI/AI)\n",
        "        #print('WI_intermediate',WI_intermediate)\n",
        "        valid_WI_index = WI_intermediate != float('inf')\n",
        "        WI[valid_WI_index] = LambertW0_vec(WI_intermediate[valid_WI_index])\n",
        "        spk[~valid_WI_index] = 2.8\n",
        "\n",
        "        spk_modified = spk.unsqueeze(1).repeat(1,nb_inputs,1) #update spk_modified, as spk changes\n",
        "       \n",
        "\n",
        "        #extend matrices to a common shape\n",
        "        AI_modified = AI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        BI_modified = BI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        WI_modified = WI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "\n",
        "        valid_weights = w1_n_bias_repeated*valid_index #only inputs contributed to the spike generation are recognised as valid inputs\n",
        "\n",
        "        dtout_dtin = grad_output_modified*valid_weights*torch.exp(x_n_bias_modified)*(x_n_bias_modified - BI_modified/AI_modified + WI_modified + 1)/(AI_modified*(1+WI_modified))\n",
        "        #no spk penalty\n",
        "        no_spk_idx = torch.where(spk_modified == 2.8)\n",
        "        dtout_dtin[no_spk_idx] = 0\n",
        "        grad_input = torch.sum(dtout_dtin,2)\n",
        "\n",
        "        dtout_dw = grad_output_modified*valid_index*torch.exp(x_n_bias_modified)*(x_n_bias_modified - BI_modified/AI_modified + WI_modified)/(AI_modified*(1+WI_modified))\n",
        "        dtout_dw[no_spk_idx] = -1 #-48.3748\n",
        "        grad_weight = torch.sum(dtout_dw,0)\n",
        "\n",
        "        ##clip at 100\n",
        "        torch.clamp(grad_weight, min=-100, max=100)\n",
        "        torch.clamp(grad_input, min=-100, max=100)\n",
        "        \n",
        "\n",
        "        return grad_input[:,:nb_inputs-n_bias], grad_weight[:nb_inputs-n_bias,:], grad_input[:,nb_inputs-n_bias:].view(batch_size,n_bias), grad_weight[nb_inputs-n_bias:,:].view(n_bias,nb_outputs) #I ignored the grad for the bias\n",
        "\n",
        "snn_process_v3 = SNN_process_v3.apply"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeq_GsruYm_g",
        "colab_type": "text"
      },
      "source": [
        "# Store the data in a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2os--Upf3tvQ",
        "colab_type": "code",
        "outputId": "4930c69e-59b3-46e3-911c-9e254fc41ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load training data for training\n",
        "import time\n",
        "batch_size = 100\n",
        "x_batch =[]\n",
        "y_batch =[]\n",
        "t0 = time.time()\n",
        "for x_local, y_local in dense_data_generator(training_addrs, training_labels, batch_size, shuffle=False):\n",
        "  x_batch.append(x_local)\n",
        "  y_batch.append(y_local)\n",
        "t1 = time.time()\n",
        "print(t1-t0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58.52047109603882\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMhDf_dy4mYl",
        "colab_type": "code",
        "outputId": "e6f889b1-1946-4b97-e60d-dba347be5f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#load trainig data for accuracy\n",
        "for x_test, y_test in dense_data_generator(test_addrs, test_labels, len(test_addrs), shuffle=False):\n",
        "  1 == 1\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 2312])\n",
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5RhokqyYuaW",
        "colab_type": "text"
      },
      "source": [
        "# Classification accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62FW9Mdeure8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    spk_hidden = snn_process_v3(x_data, w1, bias1_time, bias1_weight)\n",
        "    spk_out = snn_process_v3(spk_hidden, w2, bias2_time, bias2_weight)\n",
        "    pred = torch.argmin(spk_out,1)\n",
        "    acc = np.mean((y_data==pred).detach().cpu().numpy())\n",
        "    accs.append(acc)\n",
        "    return np.mean(accs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMMPMUtpyWXl",
        "colab_type": "code",
        "outputId": "ae71bcb0-1823-4d11-af9d-1dd55d75800e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import time \n",
        "t0 = time.time()\n",
        "n_batch = int(1000/batch_size)\n",
        "acc_list = []\n",
        "for i in range(n_batch):\n",
        "  acc = compute_classification_accuracy(x_batch[i],y_batch[i])\n",
        "  acc_list.append(acc)\n",
        "train_accuracy = np.mean(acc_list)\n",
        "t1 = time.time()\n",
        "print('time=', t1-t0)\n",
        "t0 = time.time()\n",
        "test_accuracy = compute_classification_accuracy(x_test,y_test)\n",
        "t1 = time.time()\n",
        "print('time=', t1-t0)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time= 7.759378433227539\n",
            "time= 0.7841007709503174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J--4IT-u5zvO",
        "colab_type": "code",
        "outputId": "bc7077d9-287a-4a71-ff6f-e85ab03024e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('train_accuracy', train_accuracy)\n",
        "print('test_accuracy', test_accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_accuracy 0.11500000000000002\n",
            "test_accuracy 0.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNULZU2lbTH-",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tca7YtDCqobA",
        "colab_type": "code",
        "outputId": "737a38d4-0578-4a1a-cb99-f9dedc148e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nb_inputs  = 2*34*34\n",
        "nb_hidden  = 100\n",
        "nb_outputs = 10\n",
        "#we assume there is only 1 batch now\n",
        "\n",
        "#weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "dtype=torch.float\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "#initialisation\n",
        "weight_scale = np.sqrt(2)\n",
        "\n",
        "nb_bias1 = 1\n",
        "nb_bias2 = 1\n",
        "\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=7.83912*weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias1), std=weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias1)) #the mean is brought up to 1.5 to force neurons to spike initially\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=7.83912*weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias2), std=weight_scale/np.sqrt(nb_hidden+nb_outputs+nb_bias2))\n",
        "\n",
        "bias1_time = torch.empty((nb_bias1),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.uniform_(bias1_time, a=0.0, b=1.0)\n",
        "bias1_weight = torch.empty((nb_bias1, nb_hidden), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(bias1_weight, mean=-0.275419*weight_scale/np.sqrt(nb_inputs+nb_bias1+nb_hidden), std=weight_scale/np.sqrt(nb_inputs+nb_bias1+nb_hidden))\n",
        "#torch.nn.init.uniform_(bias1_weight, a=1.0, b=1.0)\n",
        "\n",
        "\n",
        "bias2_time = torch.empty((nb_bias2),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.uniform_(bias2_time, a=0.0, b=1.0)\n",
        "bias2_weight = torch.empty((nb_bias2, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(bias2_weight, mean=-0.275419*weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden), std=weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden))\n",
        "#torch.nn.init.uniform_(bias2_weight, a=1.0, b=1.0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2235, -0.2336, -0.0431, -0.2613, -0.0197,  0.0229,  0.1542, -0.1053,\n",
              "          0.0487,  0.0204]], device='cuda:0', requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k8pYHYHkWvs",
        "colab_type": "code",
        "outputId": "e45e0bd9-501f-4dc1-94ce-b4e2f0ea4ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "params = [w1,w2, bias1_weight, bias2_weight]\n",
        "params_time = [bias1_time, bias2_time]\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_batch = int(1000/batch_size)\n",
        "lr = 2e-3\n",
        "lr_pulse = 6e-2\n",
        "\n",
        "loss_list = []\n",
        "loss_hist = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "pred_hist = torch.zeros(1000,3,2)\n",
        "\n",
        "for e in range(200):\n",
        "    i=0\n",
        "    optimizer = torch.optim.Adam(params, lr=(lr), betas=(0.9,0.999))\n",
        "    optimizer_bias_time = torch.optim.Adam(params_time, lr=(lr_pulse), betas=(0.9,0.999))\n",
        "\n",
        "    #####run snn begins###\n",
        "    t0 = time.time()\n",
        "    #for x_local, y_local in dense_data_generator(training_addrs, training_labels, batch_size, shuffle=False):\n",
        "    for i in range(n_batch):\n",
        "      x_local = x_batch[i]\n",
        "      y_local = y_batch[i]\n",
        "      spk_hidden = snn_process_v3(x_local, w1, bias1_time, bias1_weight)\n",
        "      spk_out = snn_process_v3(spk_hidden, w2, bias2_time, bias2_weight)\n",
        "      #####run snn ends#####\n",
        "      #pred = torch.argmin(spk_out,1)\n",
        "      #if (y_train_batch!=pred): #for batch size =1\n",
        "      i+=1\n",
        "      loss = loss_fn(-spk_out, y_local)\n",
        "      loss_list.append(loss.item())\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      optimizer_bias_time.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      optimizer_bias_time.step()\n",
        "\n",
        "    loss_list_t=torch.FloatTensor(loss_list) \n",
        "    loss_hist.append(torch.mean(loss_list_t))\n",
        "\n",
        "    #compute accuracies\n",
        "    acc_list = []\n",
        "    for i in range(n_batch):\n",
        "      acc = compute_classification_accuracy(x_batch[i],y_batch[i])\n",
        "      acc_list.append(acc)\n",
        "    train_accuracy = np.mean(acc_list)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    test_accuracy = compute_classification_accuracy(x_test,y_test)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "\n",
        "    t1 = time.time()\n",
        "    print('epoch number: ', e, 'loss is ', loss_hist[e].item(), 'train accu is', train_accuracy, 'test accu is', test_accuracy, 'time=', t1-t0)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch number:  0 loss is  2.302488327026367 train accu is 0.143 test accu is 0.18 time= 17.32118821144104\n",
            "epoch number:  1 loss is  2.302194833755493 train accu is 0.169 test accu is 0.18 time= 17.402410745620728\n",
            "epoch number:  2 loss is  2.3020200729370117 train accu is 0.189 test accu is 0.24 time= 17.335115432739258\n",
            "epoch number:  3 loss is  2.3018076419830322 train accu is 0.23000000000000004 test accu is 0.28 time= 17.610063791275024\n",
            "epoch number:  4 loss is  2.301502227783203 train accu is 0.259 test accu is 0.32 time= 17.419644832611084\n",
            "epoch number:  5 loss is  2.301185131072998 train accu is 0.30799999999999994 test accu is 0.34 time= 17.40800929069519\n",
            "epoch number:  6 loss is  2.3008179664611816 train accu is 0.35 test accu is 0.38 time= 17.399346590042114\n",
            "epoch number:  7 loss is  2.3003692626953125 train accu is 0.366 test accu is 0.4 time= 17.563453674316406\n",
            "epoch number:  8 loss is  2.299847364425659 train accu is 0.375 test accu is 0.4 time= 17.607072591781616\n",
            "epoch number:  9 loss is  2.2992358207702637 train accu is 0.36500000000000005 test accu is 0.41 time= 17.4576632976532\n",
            "epoch number:  10 loss is  2.2985336780548096 train accu is 0.361 test accu is 0.37 time= 17.596792936325073\n",
            "epoch number:  11 loss is  2.2976787090301514 train accu is 0.362 test accu is 0.41 time= 17.46137499809265\n",
            "epoch number:  12 loss is  2.2966384887695312 train accu is 0.351 test accu is 0.37 time= 17.541791677474976\n",
            "epoch number:  13 loss is  2.295306444168091 train accu is 0.346 test accu is 0.36 time= 17.490633249282837\n",
            "epoch number:  14 loss is  2.2934911251068115 train accu is 0.33899999999999997 test accu is 0.35 time= 17.732813596725464\n",
            "epoch number:  15 loss is  2.290947437286377 train accu is 0.32899999999999996 test accu is 0.33 time= 17.565703868865967\n",
            "epoch number:  16 loss is  2.2873475551605225 train accu is 0.32300000000000006 test accu is 0.33 time= 17.658291816711426\n",
            "epoch number:  17 loss is  2.283327579498291 train accu is 0.31000000000000005 test accu is 0.32 time= 17.850332021713257\n",
            "epoch number:  18 loss is  2.2786192893981934 train accu is 0.3 test accu is 0.32 time= 17.70034098625183\n",
            "epoch number:  19 loss is  2.2724087238311768 train accu is 0.30500000000000005 test accu is 0.33 time= 17.753548860549927\n",
            "epoch number:  20 loss is  2.2646799087524414 train accu is 0.312 test accu is 0.33 time= 17.996482372283936\n",
            "epoch number:  21 loss is  2.2553234100341797 train accu is 0.351 test accu is 0.35 time= 18.020742893218994\n",
            "epoch number:  22 loss is  2.2460384368896484 train accu is 0.364 test accu is 0.36 time= 18.011175870895386\n",
            "epoch number:  23 loss is  2.2364587783813477 train accu is 0.3969999999999999 test accu is 0.39 time= 18.00722575187683\n",
            "epoch number:  24 loss is  2.227032423019409 train accu is 0.429 test accu is 0.4 time= 18.128483295440674\n",
            "epoch number:  25 loss is  2.2174878120422363 train accu is 0.477 test accu is 0.44 time= 18.1053569316864\n",
            "epoch number:  26 loss is  2.206541061401367 train accu is 0.53 test accu is 0.57 time= 18.293638229370117\n",
            "epoch number:  27 loss is  2.1954617500305176 train accu is 0.6140000000000001 test accu is 0.62 time= 18.243594884872437\n",
            "epoch number:  28 loss is  2.1837174892425537 train accu is 0.6889999999999998 test accu is 0.66 time= 18.254709005355835\n",
            "epoch number:  29 loss is  2.1704258918762207 train accu is 0.714 test accu is 0.66 time= 18.197293758392334\n",
            "epoch number:  30 loss is  2.1551191806793213 train accu is 0.748 test accu is 0.7 time= 18.37805700302124\n",
            "epoch number:  31 loss is  2.137826681137085 train accu is 0.7719999999999999 test accu is 0.72 time= 18.379110097885132\n",
            "epoch number:  32 loss is  2.120298147201538 train accu is 0.8 test accu is 0.71 time= 18.355738401412964\n",
            "epoch number:  33 loss is  2.102090835571289 train accu is 0.806 test accu is 0.73 time= 18.263234615325928\n",
            "epoch number:  34 loss is  2.0836122035980225 train accu is 0.8089999999999999 test accu is 0.72 time= 18.49082064628601\n",
            "epoch number:  35 loss is  2.06516170501709 train accu is 0.8240000000000001 test accu is 0.74 time= 18.376805782318115\n",
            "epoch number:  36 loss is  2.047107696533203 train accu is 0.817 test accu is 0.7 time= 18.313963651657104\n",
            "epoch number:  37 loss is  2.0281872749328613 train accu is 0.8320000000000001 test accu is 0.69 time= 18.4107928276062\n",
            "epoch number:  38 loss is  2.008589029312134 train accu is 0.8350000000000002 test accu is 0.69 time= 18.458658933639526\n",
            "epoch number:  39 loss is  1.9889775514602661 train accu is 0.8380000000000001 test accu is 0.66 time= 18.475863695144653\n",
            "epoch number:  40 loss is  1.9689725637435913 train accu is 0.836 test accu is 0.66 time= 18.393393516540527\n",
            "epoch number:  41 loss is  1.9494978189468384 train accu is 0.8480000000000001 test accu is 0.68 time= 18.339969396591187\n",
            "epoch number:  42 loss is  1.930275559425354 train accu is 0.8440000000000001 test accu is 0.71 time= 18.266793727874756\n",
            "epoch number:  43 loss is  1.9112225770950317 train accu is 0.85 test accu is 0.71 time= 18.503536224365234\n",
            "epoch number:  44 loss is  1.8925278186798096 train accu is 0.8550000000000001 test accu is 0.7 time= 18.305105924606323\n",
            "epoch number:  45 loss is  1.874109148979187 train accu is 0.86 test accu is 0.71 time= 18.36824131011963\n",
            "epoch number:  46 loss is  1.8561497926712036 train accu is 0.866 test accu is 0.71 time= 18.423368453979492\n",
            "epoch number:  47 loss is  1.8388030529022217 train accu is 0.865 test accu is 0.71 time= 18.5166277885437\n",
            "epoch number:  48 loss is  1.8225783109664917 train accu is 0.873 test accu is 0.69 time= 18.429383516311646\n",
            "epoch number:  49 loss is  1.8066567182540894 train accu is 0.869 test accu is 0.71 time= 18.31404733657837\n",
            "epoch number:  50 loss is  1.7908605337142944 train accu is 0.8729999999999999 test accu is 0.7 time= 18.444090604782104\n",
            "epoch number:  51 loss is  1.7762846946716309 train accu is 0.8779999999999999 test accu is 0.7 time= 18.280813932418823\n",
            "epoch number:  52 loss is  1.7624903917312622 train accu is 0.884 test accu is 0.71 time= 18.446388959884644\n",
            "epoch number:  53 loss is  1.7484455108642578 train accu is 0.8770000000000001 test accu is 0.74 time= 18.3771390914917\n",
            "epoch number:  54 loss is  1.734763503074646 train accu is 0.8850000000000001 test accu is 0.69 time= 18.52577567100525\n",
            "epoch number:  55 loss is  1.7212632894515991 train accu is 0.884 test accu is 0.7 time= 18.389331579208374\n",
            "epoch number:  56 loss is  1.7082629203796387 train accu is 0.89 test accu is 0.71 time= 18.62957215309143\n",
            "epoch number:  57 loss is  1.6959141492843628 train accu is 0.8880000000000001 test accu is 0.72 time= 18.505719423294067\n",
            "epoch number:  58 loss is  1.6838575601577759 train accu is 0.89 test accu is 0.72 time= 18.434813737869263\n",
            "epoch number:  59 loss is  1.671924114227295 train accu is 0.8879999999999999 test accu is 0.7 time= 18.40378975868225\n",
            "epoch number:  60 loss is  1.6605829000473022 train accu is 0.8939999999999999 test accu is 0.71 time= 18.496376752853394\n",
            "epoch number:  61 loss is  1.6494003534317017 train accu is 0.8949999999999999 test accu is 0.7 time= 18.59403371810913\n",
            "epoch number:  62 loss is  1.6388022899627686 train accu is 0.901 test accu is 0.72 time= 18.4557888507843\n",
            "epoch number:  63 loss is  1.6284748315811157 train accu is 0.8939999999999999 test accu is 0.69 time= 18.42532444000244\n",
            "epoch number:  64 loss is  1.6185916662216187 train accu is 0.898 test accu is 0.72 time= 18.483648777008057\n",
            "epoch number:  65 loss is  1.6089011430740356 train accu is 0.891 test accu is 0.73 time= 18.55396819114685\n",
            "epoch number:  66 loss is  1.599409818649292 train accu is 0.8950000000000001 test accu is 0.71 time= 18.449746131896973\n",
            "epoch number:  67 loss is  1.5905754566192627 train accu is 0.8960000000000001 test accu is 0.73 time= 18.542394399642944\n",
            "epoch number:  68 loss is  1.5817365646362305 train accu is 0.897 test accu is 0.75 time= 18.384222745895386\n",
            "epoch number:  69 loss is  1.5734219551086426 train accu is 0.899 test accu is 0.74 time= 18.622605562210083\n",
            "epoch number:  70 loss is  1.5652492046356201 train accu is 0.8939999999999999 test accu is 0.74 time= 18.448702096939087\n",
            "epoch number:  71 loss is  1.5569593906402588 train accu is 0.901 test accu is 0.74 time= 18.636247396469116\n",
            "epoch number:  72 loss is  1.5489327907562256 train accu is 0.8969999999999999 test accu is 0.75 time= 18.512552738189697\n",
            "epoch number:  73 loss is  1.540966272354126 train accu is 0.898 test accu is 0.75 time= 18.44647717475891\n",
            "epoch number:  74 loss is  1.533230185508728 train accu is 0.9020000000000001 test accu is 0.75 time= 18.49425506591797\n",
            "epoch number:  75 loss is  1.5258420705795288 train accu is 0.901 test accu is 0.74 time= 18.428064107894897\n",
            "epoch number:  76 loss is  1.5183625221252441 train accu is 0.9029999999999999 test accu is 0.75 time= 18.501248359680176\n",
            "epoch number:  77 loss is  1.5111600160598755 train accu is 0.893 test accu is 0.75 time= 18.561005115509033\n",
            "epoch number:  78 loss is  1.5039867162704468 train accu is 0.9049999999999999 test accu is 0.76 time= 18.59000587463379\n",
            "epoch number:  79 loss is  1.4972233772277832 train accu is 0.8940000000000001 test accu is 0.78 time= 18.48677396774292\n",
            "epoch number:  80 loss is  1.490393042564392 train accu is 0.901 test accu is 0.78 time= 18.55506157875061\n",
            "epoch number:  81 loss is  1.483733057975769 train accu is 0.898 test accu is 0.79 time= 18.47337770462036\n",
            "epoch number:  82 loss is  1.477296233177185 train accu is 0.9030000000000001 test accu is 0.8 time= 18.667526721954346\n",
            "epoch number:  83 loss is  1.471312165260315 train accu is 0.9029999999999999 test accu is 0.79 time= 18.52991223335266\n",
            "epoch number:  84 loss is  1.4652057886123657 train accu is 0.9030000000000001 test accu is 0.78 time= 18.55764627456665\n",
            "epoch number:  85 loss is  1.459238886833191 train accu is 0.9 test accu is 0.79 time= 18.60391926765442\n",
            "epoch number:  86 loss is  1.4532279968261719 train accu is 0.9099999999999999 test accu is 0.79 time= 18.68160605430603\n",
            "epoch number:  87 loss is  1.447357416152954 train accu is 0.906 test accu is 0.79 time= 18.806371450424194\n",
            "epoch number:  88 loss is  1.4414993524551392 train accu is 0.907 test accu is 0.8 time= 18.61218786239624\n",
            "epoch number:  89 loss is  1.4358551502227783 train accu is 0.9040000000000001 test accu is 0.79 time= 18.543161392211914\n",
            "epoch number:  90 loss is  1.4303942918777466 train accu is 0.9109999999999999 test accu is 0.79 time= 18.623369932174683\n",
            "epoch number:  91 loss is  1.4248762130737305 train accu is 0.906 test accu is 0.77 time= 18.717530012130737\n",
            "epoch number:  92 loss is  1.4192359447479248 train accu is 0.9120000000000001 test accu is 0.81 time= 18.673585891723633\n",
            "epoch number:  93 loss is  1.4138941764831543 train accu is 0.9110000000000001 test accu is 0.78 time= 18.692443132400513\n",
            "epoch number:  94 loss is  1.4087228775024414 train accu is 0.917 test accu is 0.81 time= 18.804522037506104\n",
            "epoch number:  95 loss is  1.4037693738937378 train accu is 0.907 test accu is 0.8 time= 18.853935956954956\n",
            "epoch number:  96 loss is  1.3987815380096436 train accu is 0.9189999999999999 test accu is 0.8 time= 18.77086615562439\n",
            "epoch number:  97 loss is  1.3937386274337769 train accu is 0.9100000000000001 test accu is 0.8 time= 18.850337266921997\n",
            "epoch number:  98 loss is  1.3890254497528076 train accu is 0.9109999999999999 test accu is 0.77 time= 18.731412649154663\n",
            "epoch number:  99 loss is  1.384304165840149 train accu is 0.915 test accu is 0.77 time= 18.874009132385254\n",
            "epoch number:  100 loss is  1.3796629905700684 train accu is 0.9120000000000001 test accu is 0.77 time= 18.81687569618225\n",
            "epoch number:  101 loss is  1.3750333786010742 train accu is 0.9120000000000001 test accu is 0.77 time= 18.737587690353394\n",
            "epoch number:  102 loss is  1.3703333139419556 train accu is 0.914 test accu is 0.78 time= 18.95646071434021\n",
            "epoch number:  103 loss is  1.366066813468933 train accu is 0.9120000000000001 test accu is 0.75 time= 19.200230836868286\n",
            "epoch number:  104 loss is  1.3619470596313477 train accu is 0.915 test accu is 0.77 time= 19.472219944000244\n",
            "epoch number:  105 loss is  1.3575987815856934 train accu is 0.913 test accu is 0.76 time= 19.276527404785156\n",
            "epoch number:  106 loss is  1.353501558303833 train accu is 0.915 test accu is 0.76 time= 19.162854433059692\n",
            "epoch number:  107 loss is  1.349326729774475 train accu is 0.915 test accu is 0.77 time= 19.331243991851807\n",
            "epoch number:  108 loss is  1.3454785346984863 train accu is 0.916 test accu is 0.78 time= 19.694676160812378\n",
            "epoch number:  109 loss is  1.3416707515716553 train accu is 0.9099999999999999 test accu is 0.76 time= 19.67465090751648\n",
            "epoch number:  110 loss is  1.338019609451294 train accu is 0.9100000000000001 test accu is 0.75 time= 19.52462387084961\n",
            "epoch number:  111 loss is  1.3343371152877808 train accu is 0.917 test accu is 0.77 time= 19.623164176940918\n",
            "epoch number:  112 loss is  1.3306909799575806 train accu is 0.914 test accu is 0.74 time= 19.49765658378601\n",
            "epoch number:  113 loss is  1.3271164894104004 train accu is 0.914 test accu is 0.76 time= 19.522846221923828\n",
            "epoch number:  114 loss is  1.3234525918960571 train accu is 0.914 test accu is 0.76 time= 19.548033237457275\n",
            "epoch number:  115 loss is  1.3199816942214966 train accu is 0.917 test accu is 0.79 time= 19.422935962677002\n",
            "epoch number:  116 loss is  1.3165444135665894 train accu is 0.9120000000000001 test accu is 0.77 time= 19.849530458450317\n",
            "epoch number:  117 loss is  1.3132814168930054 train accu is 0.9189999999999999 test accu is 0.75 time= 19.015531301498413\n",
            "epoch number:  118 loss is  1.3100470304489136 train accu is 0.915 test accu is 0.77 time= 19.775218963623047\n",
            "epoch number:  119 loss is  1.3069123029708862 train accu is 0.916 test accu is 0.77 time= 19.207756280899048\n",
            "epoch number:  120 loss is  1.3037508726119995 train accu is 0.916 test accu is 0.79 time= 19.826183319091797\n",
            "epoch number:  121 loss is  1.3007066249847412 train accu is 0.916 test accu is 0.77 time= 19.37288188934326\n",
            "epoch number:  122 loss is  1.2975703477859497 train accu is 0.916 test accu is 0.77 time= 19.80793023109436\n",
            "epoch number:  123 loss is  1.294813632965088 train accu is 0.914 test accu is 0.78 time= 19.551912784576416\n",
            "epoch number:  124 loss is  1.2920169830322266 train accu is 0.916 test accu is 0.78 time= 19.581290245056152\n",
            "epoch number:  125 loss is  1.2891470193862915 train accu is 0.914 test accu is 0.76 time= 19.571987867355347\n",
            "epoch number:  126 loss is  1.2863545417785645 train accu is 0.914 test accu is 0.77 time= 19.435354471206665\n",
            "epoch number:  127 loss is  1.2834177017211914 train accu is 0.9179999999999999 test accu is 0.76 time= 19.551793336868286\n",
            "epoch number:  128 loss is  1.2804726362228394 train accu is 0.9169999999999998 test accu is 0.76 time= 19.758392095565796\n",
            "epoch number:  129 loss is  1.2777715921401978 train accu is 0.915 test accu is 0.76 time= 19.77410578727722\n",
            "epoch number:  130 loss is  1.2751320600509644 train accu is 0.9179999999999999 test accu is 0.77 time= 19.572030305862427\n",
            "epoch number:  131 loss is  1.2723667621612549 train accu is 0.9120000000000001 test accu is 0.75 time= 19.6768000125885\n",
            "epoch number:  132 loss is  1.2697784900665283 train accu is 0.921 test accu is 0.78 time= 19.6374831199646\n",
            "epoch number:  133 loss is  1.267325520515442 train accu is 0.916 test accu is 0.75 time= 19.681086778640747\n",
            "epoch number:  134 loss is  1.2647887468338013 train accu is 0.9149999999999998 test accu is 0.75 time= 19.717284440994263\n",
            "epoch number:  135 loss is  1.262191653251648 train accu is 0.915 test accu is 0.76 time= 19.37711763381958\n",
            "epoch number:  136 loss is  1.25985586643219 train accu is 0.914 test accu is 0.77 time= 19.757726430892944\n",
            "epoch number:  137 loss is  1.25749933719635 train accu is 0.914 test accu is 0.79 time= 19.60747766494751\n",
            "epoch number:  138 loss is  1.2553523778915405 train accu is 0.916 test accu is 0.76 time= 19.77173948287964\n",
            "epoch number:  139 loss is  1.2532132863998413 train accu is 0.914 test accu is 0.78 time= 19.69821310043335\n",
            "epoch number:  140 loss is  1.2508429288864136 train accu is 0.9179999999999999 test accu is 0.78 time= 19.40680170059204\n",
            "epoch number:  141 loss is  1.248412013053894 train accu is 0.917 test accu is 0.76 time= 19.19281530380249\n",
            "epoch number:  142 loss is  1.2461427450180054 train accu is 0.917 test accu is 0.77 time= 19.10355305671692\n",
            "epoch number:  143 loss is  1.2439441680908203 train accu is 0.9190000000000002 test accu is 0.77 time= 19.117578506469727\n",
            "epoch number:  144 loss is  1.2416311502456665 train accu is 0.9200000000000002 test accu is 0.77 time= 19.24889612197876\n",
            "epoch number:  145 loss is  1.2396236658096313 train accu is 0.9190000000000002 test accu is 0.76 time= 19.235110998153687\n",
            "epoch number:  146 loss is  1.2376762628555298 train accu is 0.916 test accu is 0.78 time= 19.120460033416748\n",
            "epoch number:  147 loss is  1.2357087135314941 train accu is 0.917 test accu is 0.76 time= 19.015865087509155\n",
            "epoch number:  148 loss is  1.2336293458938599 train accu is 0.917 test accu is 0.76 time= 19.07742667198181\n",
            "epoch number:  149 loss is  1.2316670417785645 train accu is 0.917 test accu is 0.76 time= 19.223015785217285\n",
            "epoch number:  150 loss is  1.2297629117965698 train accu is 0.9169999999999998 test accu is 0.76 time= 18.971553564071655\n",
            "epoch number:  151 loss is  1.2278443574905396 train accu is 0.9179999999999999 test accu is 0.77 time= 19.10692548751831\n",
            "epoch number:  152 loss is  1.2259974479675293 train accu is 0.923 test accu is 0.75 time= 19.323895931243896\n",
            "epoch number:  153 loss is  1.2240668535232544 train accu is 0.9179999999999999 test accu is 0.74 time= 19.243812084197998\n",
            "epoch number:  154 loss is  1.2222681045532227 train accu is 0.921 test accu is 0.75 time= 19.19774317741394\n",
            "epoch number:  155 loss is  1.2204322814941406 train accu is 0.9200000000000002 test accu is 0.76 time= 19.103575229644775\n",
            "epoch number:  156 loss is  1.2185041904449463 train accu is 0.922 test accu is 0.76 time= 19.20546793937683\n",
            "epoch number:  157 loss is  1.216651201248169 train accu is 0.922 test accu is 0.73 time= 19.11781120300293\n",
            "epoch number:  158 loss is  1.2148393392562866 train accu is 0.924 test accu is 0.76 time= 19.0716073513031\n",
            "epoch number:  159 loss is  1.2130911350250244 train accu is 0.9190000000000002 test accu is 0.77 time= 19.07295513153076\n",
            "epoch number:  160 loss is  1.211412787437439 train accu is 0.9259999999999999 test accu is 0.77 time= 19.193466186523438\n",
            "epoch number:  161 loss is  1.2096495628356934 train accu is 0.9269999999999999 test accu is 0.74 time= 19.191049098968506\n",
            "epoch number:  162 loss is  1.2079695463180542 train accu is 0.921 test accu is 0.76 time= 19.229594230651855\n",
            "epoch number:  163 loss is  1.206346035003662 train accu is 0.921 test accu is 0.74 time= 19.128333568572998\n",
            "epoch number:  164 loss is  1.2046867609024048 train accu is 0.9189999999999999 test accu is 0.77 time= 19.077810525894165\n",
            "epoch number:  165 loss is  1.2030818462371826 train accu is 0.922 test accu is 0.73 time= 19.111459016799927\n",
            "epoch number:  166 loss is  1.201387643814087 train accu is 0.925 test accu is 0.75 time= 19.2315092086792\n",
            "epoch number:  167 loss is  1.1996381282806396 train accu is 0.9190000000000002 test accu is 0.74 time= 19.129789352416992\n",
            "epoch number:  168 loss is  1.1981980800628662 train accu is 0.9270000000000002 test accu is 0.76 time= 19.35729479789734\n",
            "epoch number:  169 loss is  1.1965999603271484 train accu is 0.925 test accu is 0.74 time= 19.45534348487854\n",
            "epoch number:  170 loss is  1.1950842142105103 train accu is 0.9260000000000002 test accu is 0.73 time= 19.31057119369507\n",
            "epoch number:  171 loss is  1.1938544511795044 train accu is 0.9170000000000001 test accu is 0.75 time= 19.087327480316162\n",
            "epoch number:  172 loss is  1.1926378011703491 train accu is 0.923 test accu is 0.76 time= 19.087823152542114\n",
            "epoch number:  173 loss is  1.1912388801574707 train accu is 0.9200000000000002 test accu is 0.74 time= 19.07996106147766\n",
            "epoch number:  174 loss is  1.1900124549865723 train accu is 0.9200000000000002 test accu is 0.75 time= 19.24680757522583\n",
            "epoch number:  175 loss is  1.1888903379440308 train accu is 0.925 test accu is 0.74 time= 19.146418571472168\n",
            "epoch number:  176 loss is  1.187613606452942 train accu is 0.925 test accu is 0.74 time= 19.308165550231934\n",
            "epoch number:  177 loss is  1.1861188411712646 train accu is 0.925 test accu is 0.74 time= 19.09264874458313\n",
            "epoch number:  178 loss is  1.1845545768737793 train accu is 0.925 test accu is 0.75 time= 19.133084774017334\n",
            "epoch number:  179 loss is  1.18303382396698 train accu is 0.9269999999999999 test accu is 0.74 time= 19.229437351226807\n",
            "epoch number:  180 loss is  1.1817023754119873 train accu is 0.925 test accu is 0.74 time= 19.132513284683228\n",
            "epoch number:  181 loss is  1.1805353164672852 train accu is 0.924 test accu is 0.72 time= 19.138530492782593\n",
            "epoch number:  182 loss is  1.1796027421951294 train accu is 0.923 test accu is 0.73 time= 19.267414569854736\n",
            "epoch number:  183 loss is  1.1785674095153809 train accu is 0.925 test accu is 0.76 time= 19.272008895874023\n",
            "epoch number:  184 loss is  1.1776783466339111 train accu is 0.9259999999999999 test accu is 0.74 time= 19.34363293647766\n",
            "epoch number:  185 loss is  1.176734447479248 train accu is 0.9280000000000002 test accu is 0.73 time= 19.340351104736328\n",
            "epoch number:  186 loss is  1.1755961179733276 train accu is 0.925 test accu is 0.74 time= 19.45235562324524\n",
            "epoch number:  187 loss is  1.174235463142395 train accu is 0.9259999999999999 test accu is 0.74 time= 19.4090678691864\n",
            "epoch number:  188 loss is  1.1729687452316284 train accu is 0.9259999999999999 test accu is 0.73 time= 19.185879230499268\n",
            "epoch number:  189 loss is  1.1717218160629272 train accu is 0.9259999999999999 test accu is 0.75 time= 19.169525146484375\n",
            "epoch number:  190 loss is  1.1705727577209473 train accu is 0.9280000000000002 test accu is 0.77 time= 19.144331455230713\n",
            "epoch number:  191 loss is  1.169826865196228 train accu is 0.923 test accu is 0.74 time= 19.177229404449463\n",
            "epoch number:  192 loss is  1.168990135192871 train accu is 0.9280000000000002 test accu is 0.76 time= 19.318427562713623\n",
            "epoch number:  193 loss is  1.167915940284729 train accu is 0.9280000000000002 test accu is 0.77 time= 19.171544075012207\n",
            "epoch number:  194 loss is  1.1666747331619263 train accu is 0.9269999999999999 test accu is 0.74 time= 19.19060468673706\n",
            "epoch number:  195 loss is  1.1652061939239502 train accu is 0.9269999999999999 test accu is 0.74 time= 19.359828233718872\n",
            "epoch number:  196 loss is  1.1639517545700073 train accu is 0.9289999999999999 test accu is 0.74 time= 19.327794075012207\n",
            "epoch number:  197 loss is  1.1627507209777832 train accu is 0.9289999999999999 test accu is 0.76 time= 19.303016185760498\n",
            "epoch number:  198 loss is  1.1616820096969604 train accu is 0.9339999999999999 test accu is 0.73 time= 19.248597383499146\n",
            "epoch number:  199 loss is  1.1609693765640259 train accu is 0.925 test accu is 0.75 time= 19.316452026367188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq3kMskLyttW",
        "colab_type": "code",
        "outputId": "05162439-40ee-4ed5-8ba7-5df6aea45392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot (train_accuracies,  label='train')\n",
        "plt.plot (test_accuracies, label='test')\n",
        "plt.legend()\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.title('Accuracy')\n",
        "#plt.savefig('accuracy.eps')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}