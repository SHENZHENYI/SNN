{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Concatenation of STNN and TSNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_QwdIi-6YZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "with zipfile.ZipFile('NMNISTsmall.zip') as zip_file:\n",
        "    for member in zip_file.namelist():\n",
        "        if not os.path.exists('./' + member):\n",
        "            zip_file.extract(member, './')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hniVzyg6wbV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "26cdcbd7-534c-4919-91c1-5320ab36505f"
      },
      "source": [
        "import numpy as np\n",
        "import spikeFileIO as io\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muM5E8jD77Od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#lets collect addresses of training and test data\n",
        "def load(fname):\n",
        "    ''' Load the file using std open'''\n",
        "    f = open(fname,'r')\n",
        "\n",
        "    data = []\n",
        "    for line in f.readlines():\n",
        "        data.append(line.replace('\\n','').split(' '))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "path = 'NMNISTsmall/'\n",
        "#train data\n",
        "training_files = load('NMNISTsmall/train1K.txt')\n",
        "training_files = training_files[1:]\n",
        "\n",
        "training_addrs = []\n",
        "training_labels = []\n",
        "for i in range (len(training_files)):\n",
        "  training_addrs.append(path+training_files[i][0][:-2]+'.bs2')\n",
        "  training_labels.append(training_files[i][0][-1])\n",
        "\n",
        "#test data\n",
        "test_files = load('NMNISTsmall/test100.txt')\n",
        "test_files = test_files[1:]\n",
        "\n",
        "test_addrs = []\n",
        "test_labels = []\n",
        "for i in range (len(test_files)):\n",
        "  test_addrs.append(path+test_files[i][0][:-2]+'.bs2')\n",
        "  test_labels.append(test_files[i][0][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1TBD0006yWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dense_data_generator(X, y, batch_size, samplingTime=1, samplingLength=300, shuffle=True, device = 'cuda', dtype=torch.float):\n",
        "    \"\"\" This generator takes training data's address and generates spiking network input as dense tensors. \n",
        "\n",
        "    Args:\n",
        "        X: The data ( 'data/userx_lighting_conditions/y.npy' )\n",
        "        y: The labels\n",
        "        batch_size: batch size\n",
        "        samplingTime: period of sampling; default is 1ms\n",
        "        samplingLength: in SLAYER training as well as testing, only the first 1.5 s out of â‰ˆ 6 s of action video for each clas\n",
        "    \"\"\" \n",
        "    nTimeBins = int(samplingLength/samplingTime)\n",
        "    labels_ = np.array(y,dtype=np.int)\n",
        "    number_of_batches = len(X)//batch_size\n",
        "    sample_index = np.arange(len(X))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(sample_index)\n",
        "\n",
        "    total_batch_count = 0\n",
        "    counter = 0\n",
        "    while counter<number_of_batches:\n",
        "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
        "        #X_batch = torch.empty((batch_size,2,34,34,nTimeBins),dtype=dtype)\n",
        "        X_batch = torch.empty((batch_size,nTimeBins,2*34*34),dtype=dtype)\n",
        "        for bc,i in enumerate(batch_index):\n",
        "            TD = io.read2Dspikes(X[i])\n",
        "            X_batch_temp = TD.toSpikeTensor(torch.zeros((2,34,34,nTimeBins),device=device),samplingTime=samplingTime)\n",
        "            #flatten dimensions\n",
        "            for j in range (nTimeBins):\n",
        "              X_batch[bc,j] = X_batch_temp[:,:,:,j].view((2*34*34))\n",
        "            #X_batch[bc] = TD.toSpikeTensor(torch.zeros((2,34,34,nTimeBins),device=device),samplingTime=samplingTime)\n",
        "        #temp=X_batch.view(batch_size,2*34*34,nTimeBins)\n",
        "        #X_batch = temp.view(batch_size,nTimeBins,2*34*34) #reshape\n",
        "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
        "        yield [X_batch.to(device=device), y_batch.to(device=device)] \n",
        "        counter+=1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQqi-j_cihlv",
        "colab_type": "text"
      },
      "source": [
        "Firstly, we define the surrogate gradient in STNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrksNWnyW46T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_inputs  = 2*34*34\n",
        "nb_hidden  = 200\n",
        "nb_outputs = 10\n",
        "\n",
        "time_step = 1 #sampling rate\n",
        "nb_steps  = 500 #total span of time\n",
        "\n",
        "batch_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phQ6f17iW5f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tau_mem = 10 \n",
        "tau_syn = 5\n",
        "\n",
        "alpha   = float(np.exp(-time_step/tau_syn))\n",
        "beta    = float(np.exp(-time_step/tau_mem))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk3S1GTNW_Vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b89f77d-0be7-4fd7-f357-9509c4537268"
      },
      "source": [
        "dtype=torch.float\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale/np.sqrt(nb_hidden))\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASksgyw-XGym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SurrGradSpike(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements \n",
        "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid \n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "    \n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which \n",
        "        we need to later backpropagate our error signals. To achieve this we use the \n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        out = torch.zeros_like(input)\n",
        "        out[input > 0] = 1.0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the \n",
        "        surrogate gradient of the loss with respect to the input. \n",
        "        Here we use the normalized negative part of a fast sigmoid \n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
        "        return grad\n",
        "    \n",
        "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
        "spike_fn  = SurrGradSpike.apply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apvSfPyii0O5",
        "colab_type": "text"
      },
      "source": [
        "here, we define TSNN dynamics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd3Q5qQiA0JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def alpha_kernel_response_temporal_v2(input_time, weights, tau = 1, thre = 1): #faster implementation\n",
        "  #argument: input_time shape = (n_batch x n_input);  weights shape = (n_input x n_output)\n",
        "  #alpha_kernel_response is t*e^(tau*t); the increment of this kernel is (1+t*tau)*exp(tau*t)\n",
        "  n_batch = input_time.shape[0]\n",
        "  n_input = input_time.shape[1]\n",
        "  n_output = weights.shape[1]\n",
        "  precision = 100.0\n",
        "  index = torch.where(input_time != 2.8)\n",
        "  t_max = torch.max(input_time[index]) + 4.0 #4 is the designed kernel length\n",
        "  n_max = precision*t_max\n",
        "\n",
        "  v_accu = torch.zeros((n_batch, n_output), device = input_time.device)\n",
        "  spike_time = torch.zeros_like(v_accu)\n",
        "  spike_time_mem = torch.ones_like(v_accu)\n",
        "  for t in range(int(n_max)):\n",
        "    t_clamp = torch.clamp(t/precision -input_time, 0, t_max)\n",
        "    invalid_incr = (t_clamp==0)\n",
        "    v_incr = torch.exp(-tau*t_clamp)*(1-tau*t_clamp) #unweighted dv/dt\n",
        "    v_incr[invalid_incr] = 0\n",
        "    v_incr = torch.mm(v_incr, weights)/precision #dv/dt * delta_t\n",
        "    v_accu += v_incr #update the membrane potential at time instance t\n",
        "    index = (v_accu>thre) & (spike_time_mem == 1) #output index to update the earilest spiking time\n",
        "    spike_time[index] = t/precision\n",
        "    spike_time_mem[index] = 0 #mark the updated output index such that it will not be updated again\n",
        "  \n",
        "  index_nospike = (spike_time == 0)\n",
        "  spike_time[index_nospike] = 2.8 #an arbirary number that identifies no-spiking neuron\n",
        "  \n",
        "  return spike_time.to(input_time.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXz5g4bVA5i1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AInBI(weights, spk_time, tau =1):\n",
        "  #Argument: \n",
        "  #weights dim = (n_input_neurons, n_output_neurons)\n",
        "  #spk_time dim = (n_time_units, n_input_neurons)\n",
        "  spk_time=spk_time.view((spk_time.shape[0],1))\n",
        "\n",
        "  AI = torch.sum(torch.exp(spk_time*tau)*weights, 0).to(spk_time.device)\n",
        "  #AI = AI + torch.exp(bias_time)\n",
        "  BI = torch.sum(torch.exp(spk_time*tau)*spk_time*weights, 0).to(spk_time.device)\n",
        "  #BI = BI + torch.exp(bias_time)*bias_time\n",
        "  return AI, BI"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZr6BZQuA6iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#https://github.com/google/ihmehimmeli/blob/master/tempcoding/lambertw.cc\n",
        "def LambertW0InitialGuess_vec(x):\n",
        "  kNearBranchCutoff = -0.3235\n",
        "  kE = 2.718281828459045\n",
        "  x_copy = x.clone()\n",
        "\n",
        "  # Sqrt approximation near branch cutoff.\n",
        "  index1 = (x < kNearBranchCutoff)\n",
        "  x_copy[index1] = -1.0+torch.sqrt(2.0*(1+kE*x[index1]))\n",
        "\n",
        "  # Taylor series between [-1/e and 1/e].\n",
        "  index2 = (x > kNearBranchCutoff) & (x < -kNearBranchCutoff)\n",
        "  x_copy[index2] = x[index2] * (1 + x[index2] * (-1 + x[index2] * (3.0 / 2.0 - 8.0 / 3.0 * x[index2])))\n",
        "\n",
        "  #Series of piecewise linear approximation\n",
        "  index3 = (x > -kNearBranchCutoff) & (x < 0.6)\n",
        "  x_copy[index3] =  0.23675531078855933 + (x[index3] - 0.3) * 0.5493610866617109\n",
        "  \n",
        "  index4 = (x > 0.6) & (x < 0.8999999999999999)\n",
        "  x_copy[index4] = 0.4015636367870726 + (x[index4] - 0.6) * 0.4275644294878729\n",
        "\n",
        "  index5 = (x > 0.8999999999999999) & (x < 1.2)\n",
        "  x_copy[index5] = 0.5298329656334344 + (x[index5] - 0.8999999999999999) * 0.3524368357714513\n",
        "\n",
        "  index6 = (x > 1.2) & (x < 1.5)\n",
        "  x_copy[index6] = 0.6355640163648698 + (x[index6] - 1.2) * 0.30099113800452154\n",
        "\n",
        "  index7 = (x > 1.5) & (x < 1.8)\n",
        "  x_copy[index7] = 0.7258613577662263 + (x[index7] - 1.5) * 0.2633490154764343\n",
        "  \n",
        "  index8 = (x > 1.8) & (x < 2.0999999999999996)\n",
        "  x_copy[index8] = 0.8048660624091566 + (x[index8] - 1.8) * 0.2345089875713013;\n",
        "  \n",
        "  index9 = (x > 2.0999999999999996) & (x < 2.4)\n",
        "  x_copy[index9] =  0.8752187586805469 + (x[index9] - 2.0999999999999996) * 0.2116494532726034\n",
        "\n",
        "  index10 = (x > 2.4) & (x < 2.6999999999999997)\n",
        "  x_copy[index10] = 0.938713594662328 + (x[index10] - 2.4) * 0.19305046534383152\n",
        "\n",
        "  index11 = (x > 2.6999999999999997) & (x < 2.9999999999999996)\n",
        "  x_copy[index11] = 0.9966287342654774 + (x[index11] - 2.6999999999999997) * 0.17760053566187495\n",
        "  \n",
        "  #asymptotic approxiamtion\n",
        "  index12 = ~(index1 + index2 + index3 + index4 + index5 + index6 +index7 +index8 +index9 +index10 + index11)\n",
        "  l = torch.log(x[index12])\n",
        "  ll = torch.log(l)\n",
        "  x_copy[index12] = l - ll + ll/l\n",
        "  return x_copy\n",
        "\n",
        "def LambertW0_vec(x):\n",
        "  x_copy = x.clone()\n",
        "  kReciprocalE = 0.36787944117\n",
        "  kDesiredAbsoluteDifference = 1e-3\n",
        "  kNumMaxIters = 10\n",
        "\n",
        "  index1 =  (x < -kReciprocalE)\n",
        "  x_copy[index1] = 0\n",
        "  #return x, False\n",
        "\n",
        "  index2 = (x == 0.0)\n",
        "  x_copy[index2] = 0\n",
        "\n",
        "  index3 = (x == -kReciprocalE)\n",
        "  x_copy[index3] = -1 \n",
        "\n",
        "  index4 = ~(index1 + index2 + index3)\n",
        "\n",
        "  #current guess\n",
        "  w_n = LambertW0InitialGuess_vec(x[index4])\n",
        "  have_convergence = False\n",
        "\n",
        "  #fritsch iteration\n",
        "  for i in range(1):\n",
        "    z_n = torch.log(x[index4] / w_n) - w_n\n",
        "    q_n = 2.0 * (1.0 + w_n) * (1.0 + w_n + 2.0 / 3.0 * z_n)\n",
        "    e_n = (z_n / (1.0 + w_n)) * ((q_n - z_n) / (q_n - 2.0 * z_n))\n",
        "    w_n = w_n * (1.0 + e_n)\n",
        "    #Done this way as the log is the expensive part above.\n",
        "    #if (torch.abs(z_n) < kDesiredAbsoluteDifference):\n",
        "\n",
        "  x_copy[index4] = w_n\n",
        "  return x_copy\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FR6e0P4VA9h0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SNN_process_v3(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, weight, bias_time, bias_weight):\n",
        "        \"\"\"\n",
        "        Argument: input is input spike time of dimension (n_output)\n",
        "                  weight is of dimension (n_input, n_output)\n",
        "                  bias_time is of dimension (n_bias)\n",
        "                  bias_weight is of dimension (n_bias, n_output)\n",
        "        \"\"\"\n",
        "        #####firstly, we combine the bias and inputs, and their weights#####\n",
        "        batch_size = input.shape[0]\n",
        "        n_bias = bias_time.shape[0]\n",
        "        bias_time_broadcasted = bias_time.view(1,n_bias).repeat(batch_size,1)\n",
        "        x_n_bias = torch.cat((input, bias_time_broadcasted),-1)\n",
        "\n",
        "        w_n_bias = torch.cat((weight,bias_weight),0)\n",
        "\n",
        "        #####membrane dynamics begin here###################################\n",
        "        tau =  2#0.181769\n",
        "        theta = 0.5 #1.16732\n",
        "        not_spike_time = 2.8 #if the neuron does not spike, the spike time is assigned to 2.8 sec. It is a potential problem\n",
        "        nb_outputs = w_n_bias.shape[1]\n",
        "\n",
        "        spk = alpha_kernel_response_temporal_v2(x_n_bias, w_n_bias, tau = tau, thre = theta)\n",
        "        ctx.intermediate_results = x_n_bias, w_n_bias, spk, n_bias\n",
        "\n",
        "        return spk\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        \"\"\"\n",
        "        x_n_bias, w1_n_bias, spk, n_bias = ctx.intermediate_results\n",
        "        tau = 2 #0.181769\n",
        "        theta = 0.5 #1.16732\n",
        "        input = x_n_bias\n",
        "        batch_size = x_n_bias.shape[0]\n",
        "        nb_inputs = w1_n_bias.shape[0]\n",
        "        nb_outputs = w1_n_bias.shape[1]\n",
        "\n",
        "        #t0 = time.time()\n",
        "        #compute ingredients of gradients \n",
        "        AI = torch.zeros_like(spk).to(input.device)  #initialisation\n",
        "        BI = torch.zeros_like(spk).to(input.device)\n",
        "        t_out = torch.zeros_like(spk).to(input.device)\n",
        "        WI = torch.zeros_like(spk).to(input.device)\n",
        "        grad_input = torch.zeros((batch_size, nb_inputs, nb_outputs)).to(input.device)\n",
        "        grad_weight = torch.zeros((batch_size, nb_inputs, nb_outputs)).to(input.device)\n",
        "\n",
        "        #extend matrices to a common shape\n",
        "        x_n_bias_modified = x_n_bias.unsqueeze(2).repeat(1,1,nb_outputs)\n",
        "        grad_output_modified = grad_output.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        w1_n_bias_repeated = w1_n_bias.unsqueeze(0).repeat(batch_size,1,1)\n",
        "\n",
        "        #find the index for valid inputs \n",
        "        spk_modified = spk.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        valid_index = x_n_bias_modified < spk_modified\n",
        "\n",
        "        #print(valid_index)\n",
        "        AI = torch.sum(torch.exp(x_n_bias_modified*tau)*w1_n_bias_repeated*valid_index, 1).to(x_n_bias.device)\n",
        "        BI = torch.sum(torch.exp(x_n_bias_modified*tau)*x_n_bias_modified*w1_n_bias_repeated*valid_index, 1).to(x_n_bias.device)\n",
        "        #t1 = time.time()\n",
        "        #print('AI and BI', t1-t0)\n",
        "        exploding_WI_idx_batch = []\n",
        "        exploding_WI_idx_out = []\n",
        "\n",
        "        WI_intermediate = -tau*theta/AI * torch.exp(tau*BI/AI)\n",
        "        #print('WI_intermediate',WI_intermediate)\n",
        "        valid_WI_index = WI_intermediate != float('inf')\n",
        "        WI[valid_WI_index] = LambertW0_vec(WI_intermediate[valid_WI_index])\n",
        "        spk[~valid_WI_index] = 2.8\n",
        "\n",
        "        spk_modified = spk.unsqueeze(1).repeat(1,nb_inputs,1) #update spk_modified, as spk changes\n",
        "       \n",
        "\n",
        "        #extend matrices to a common shape\n",
        "        AI_modified = AI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        BI_modified = BI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "        WI_modified = WI.unsqueeze(1).repeat(1,nb_inputs,1)\n",
        "\n",
        "        valid_weights = w1_n_bias_repeated*valid_index #only inputs contributed to the spike generation are recognised as valid inputs\n",
        "\n",
        "        dtout_dtin = grad_output_modified*valid_weights*torch.exp(x_n_bias_modified)*(x_n_bias_modified - BI_modified/AI_modified + WI_modified + 1)/(AI_modified*(1+WI_modified))\n",
        "        #no spk penalty\n",
        "        no_spk_idx = torch.where(spk_modified == 2.8)\n",
        "        dtout_dtin[no_spk_idx] = 0\n",
        "        grad_input = torch.sum(dtout_dtin,2)\n",
        "\n",
        "        dtout_dw = grad_output_modified*valid_index*torch.exp(x_n_bias_modified)*(x_n_bias_modified - BI_modified/AI_modified + WI_modified)/(AI_modified*(1+WI_modified))\n",
        "        dtout_dw[no_spk_idx] = -1\n",
        "        grad_weight = torch.sum(dtout_dw,0)\n",
        "\n",
        "        ##clip at 100\n",
        "        torch.clamp(grad_weight, min=-100, max=100)\n",
        "        torch.clamp(grad_input, min=-100, max=100)\n",
        "\n",
        "        #solve nan values\n",
        "        index_nan_t = torch.where(grad_input != grad_input)\n",
        "        grad_input[index_nan_t] = 0\n",
        "        index_nan_w = torch.where(grad_weight != grad_weight)\n",
        "        grad_weight[index_nan_w] = 0        \n",
        "        \n",
        "        index_nan_t = torch.where(grad_input == float('inf'))\n",
        "        grad_input[index_nan_t] = 100\n",
        "        index_nan_w = torch.where(grad_weight == float('inf'))\n",
        "        grad_weight[index_nan_w] = 100  \n",
        "\n",
        "        index_nan_t = torch.where(grad_input == float('-inf'))\n",
        "        grad_input[index_nan_t] = -100\n",
        "        index_nan_w = torch.where(grad_weight == float('-inf'))\n",
        "        grad_weight[index_nan_w] = -100   \n",
        "\n",
        "        return grad_input[:,:nb_inputs-n_bias], grad_weight[:nb_inputs-n_bias,:], grad_input[:,nb_inputs-n_bias:].view(batch_size,n_bias), grad_weight[nb_inputs-n_bias:,:].view(n_bias,nb_outputs) #I ignored the grad for the bias\n",
        "\n",
        "snn_process_v3 = SNN_process_v3.apply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxRcC1H3jB-Z",
        "colab_type": "text"
      },
      "source": [
        "we combine the dynamics of STNN and TSNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myXrby_zCtGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_snn2(inputs):\n",
        "    #begin STNN\n",
        "    h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
        "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
        "\n",
        "    mem_rec = [mem]\n",
        "    spk_rec = [mem]\n",
        "\n",
        "    # Compute hidden layer activity\n",
        "    for t in range(nb_steps):\n",
        "        mthr = mem-1\n",
        "        out = spike_fn(mthr)\n",
        "        rst = torch.zeros_like(mem)\n",
        "        c   = (mthr > 0)\n",
        "        rst[c] = torch.ones_like(mem)[c]\n",
        "\n",
        "        new_syn = alpha*syn +h1[:,t]\n",
        "        new_mem = beta*mem +syn -rst\n",
        "\n",
        "        mem = new_mem\n",
        "        syn = new_syn\n",
        "\n",
        "        mem_rec.append(mem)\n",
        "        spk_rec.append(out)\n",
        "\n",
        "    mem_rec = torch.stack(mem_rec,dim=1)\n",
        "    spk_rec = torch.stack(spk_rec,dim=1)\n",
        "\n",
        "    #conversion\n",
        "    spk_time = conversion(spk_rec)\n",
        "    #begin TSNN\n",
        "    spk_out = snn_process_v3(spk_time, w2, bias2_time, bias2_weight)\n",
        "\n",
        "    return spk_out#, other_recs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_f41DqZz_Sd",
        "colab_type": "text"
      },
      "source": [
        "**LOSS** **FUNCITON**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKNukc1OXJ8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train(x_data, y_data, lr=2e-3, nb_epochs=10):\n",
        "    device = 'cuda'\n",
        "    params = [w1,w2]\n",
        "    optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999), amsgrad=True)\n",
        "    error = spike_loss(valid_region=torch.tensor([0,500]).to(device), spike_count_routine=torch.tensor([60,10]).to(device), time_step=1, device=device).to(device)\n",
        "\n",
        "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    \n",
        "    loss_hist = []\n",
        "    for e in range(nb_epochs):\n",
        "        local_loss = []\n",
        "        for x_local, y_local in dense_data_generator(x_data, y_data, batch_size):\n",
        "            zeros = torch.zeros((x_local.shape[0], 200, x_local.shape[2])).to(device)\n",
        "            x_local = torch.cat((x_local, zeros),1)\n",
        "            #print(x_local.shape)\n",
        "            x_local = Variable(x_local, requires_grad=True)\n",
        "            output,_ = run_snn(x_local)\n",
        "            loss_val = error.rate_loss(output, y_local)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_val.backward()\n",
        "            optimizer.step()\n",
        "            local_loss.append(loss_val.item())\n",
        "        mean_loss = np.mean(local_loss)\n",
        "        print(\"Epoch %i: loss=%.5f\"%(e+1,mean_loss))\n",
        "        loss_hist.append(mean_loss)\n",
        "        \n",
        "    return loss_hist, output\n",
        "        \n",
        "        \n",
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    for x_local, y_local in dense_data_generator(x_data, y_data, batch_size, shuffle=False):\n",
        "        zeros = torch.zeros((x_local.shape[0], 200, x_local.shape[2])).to(device)\n",
        "        x_local = torch.cat((x_local, zeros),1)\n",
        "        output,_ = run_snn(x_local)\n",
        "        actual_spikes = torch.sum(output,1,keepdim=True).squeeze() #count the spikes in each output neuron in each batch\n",
        "        pred = torch.argmax(actual_spikes,1)\n",
        "        tmp = np.mean((y_local==pred).detach().cpu().numpy()) # compare to labels\n",
        "        accs.append(tmp)\n",
        "    return np.mean(accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-HfHkfr89aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class spike_loss_temporary(torch.nn.Module):\n",
        "  def __init__(self, time_step, device):\n",
        "    '''\n",
        "    Arguments:\n",
        "    valid_region: 2D list [a,b]; a is the starting point in ms, and b is the end point in ms\n",
        "    spike_count_routine: 2D list [a,b]; a is the spike count for a desired class -- empirically 60 counts; b is the count for others -- empirically 10 counts\n",
        "    '''\n",
        "    super(spike_loss_temporary, self).__init__()\n",
        "    self.time_step = time_step\n",
        "    self.device = device\n",
        "\n",
        "  def temporal_loss(self,out_spike, desired_class,penalty_matrix):\n",
        "    '''\n",
        "    Arguments:\n",
        "    out_spike: dim = (n_batch, n_time, n_class)\n",
        "    desired_class: dim = (n_batch); each element is ranging from 0 to 9 as different classes\n",
        "    '''\n",
        "    penalty = torch.sum(penalty_matrix*out_spike, 1)\n",
        "    \n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    return cross_entropy(penalty, desired_class).to(self.device)\n",
        "\n",
        "  def temporal_loss_v2(self,out_spike, desired_class,penalty_matrix):\n",
        "    '''\n",
        "    Arguments:\n",
        "    out_spike: dim = (n_batch, n_time, n_class)\n",
        "    desired_class: dim = (n_batch); each element is ranging from 0 to 9 as different classes\n",
        "    '''\n",
        "    spike_times = penalty_matrix*out_spike\n",
        "    earliest_spike_times = 501 - torch.max(spike_times, 1)[0]\n",
        "    earliest_spike_times/=500\n",
        "\n",
        "    cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "    return cross_entropy(-earliest_spike_times, desired_class).to(self.device)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRtSMOrKCEn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#conversion function without a surrogatye gradient defined \n",
        "\n",
        "def conversion(out_spike):\n",
        "    n_batch = out_spike.shape[0]\n",
        "    n_time = out_spike.shape[1]\n",
        "    n_feature = out_spike.shape[2]\n",
        "\n",
        "    penalty_vector = n_time-1-torch.arange(0,n_time)\n",
        "    penalty_matrix = torch.zeros(n_batch,n_time, n_feature).to(device)\n",
        "    for i in range(n_batch):\n",
        "      for j in range(n_feature):\n",
        "        penalty_matrix[i,:,j] = penalty_vector\n",
        "\n",
        "    spike_times = penalty_matrix*out_spike\n",
        "    earliest_spike_times = n_time-1 - torch.max(spike_times, 1)[0]\n",
        "    #print(torch.sum(spike_times, 1))\n",
        "    earliest_spike_times/=(n_time-1)\n",
        "    return earliest_spike_times.to(device)\n",
        "  #have a backwar prop "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N8sz_GOf2zE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#conversion function with a surrogatye gradient defined \n",
        "class conversion2(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Here we implement our spiking nonlinearity which also implements \n",
        "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
        "    we will be able to use all of PyTorch's autograd functionality.\n",
        "    Here we use the normalized negative part of a fast sigmoid \n",
        "    as this was done in Zenke & Ganguli (2018).\n",
        "    \"\"\"\n",
        "    \n",
        "    scale = 100.0 # controls steepness of surrogate gradient\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, out_spike):\n",
        "        \"\"\"\n",
        "        In the forward pass we compute a step function of the input Tensor\n",
        "        and return it. ctx is a context object that we use to stash information which \n",
        "        we need to later backpropagate our error signals. To achieve this we use the \n",
        "        ctx.save_for_backward method.\n",
        "        \"\"\"\n",
        "\n",
        "        n_batch = out_spike.shape[0]\n",
        "        n_time = out_spike.shape[1]\n",
        "        n_feature = out_spike.shape[2]\n",
        "\n",
        "        penalty_vector = n_time-1-torch.arange(0,n_time)\n",
        "        penalty_vector = penalty_vector.unsqueeze(0)\n",
        "        penalty_vector = penalty_vector.unsqueeze(2)\n",
        "\n",
        "        penalty_matrix = penalty_vector.repeat(n_batch,1,n_feature).to(device)\n",
        "        spike_times = penalty_matrix*out_spike\n",
        "        earliest_spike_times = n_time-1 - torch.max(spike_times, 1)[0]\n",
        "\n",
        "        ctx.save_for_backward(out_spike,earliest_spike_times.long())\n",
        "\n",
        "        earliest_spike_times/=(n_time-1)\n",
        "      \n",
        "\n",
        "        return earliest_spike_times.to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor we need to compute the \n",
        "        surrogate gradient of the loss with respect to the input. \n",
        "        Here we use the normalized negative part of a fast sigmoid \n",
        "        as this was done in Zenke & Ganguli (2018).\n",
        "        \"\"\"\n",
        "        out_spike, earliest_spike_times_index = ctx.saved_tensors\n",
        "\n",
        "        n_batch = out_spike.shape[0]\n",
        "        n_time = out_spike.shape[1]\n",
        "        n_feature = out_spike.shape[2]\n",
        "\n",
        "        grad_input = grad_output.clone()\n",
        "        grad = torch.zeros_like(out_spike).to(device)\n",
        "        for i in range(n_batch):\n",
        "          for j in range(n_feature):\n",
        "            grad[i,earliest_spike_times_index[i,j],j] = grad_input[i,j]*torch.exp(-earliest_spike_times_index[i,j].double())\n",
        "        return grad\n",
        "    \n",
        "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
        "conversion23  = conversion2.apply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIri_VgKLLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_classification_accuracy(x_data, y_data):\n",
        "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
        "    accs = []\n",
        "    for x_local, y_local in dense_data_generator(x_data, y_data, batch_size, shuffle=False):\n",
        "        zeros = torch.zeros((x_local.shape[0], 200, x_local.shape[2])).to(device)\n",
        "        x_local = torch.cat((x_local, zeros),1)\n",
        "        output,_ = run_snn2(x_local)\n",
        "        pred = torch.argmin(output,1)\n",
        "        acc = np.mean((y_local==pred).detach().cpu().numpy())\n",
        "        accs.append(acc)\n",
        "    return np.mean(accs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyFvy0LLmx6S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ec8e01d-520d-4de2-8305-af3041d7e0db"
      },
      "source": [
        "dtype=torch.float\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")     \n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "nb_hidden = 400\n",
        "#nb_hidden2 = 100\n",
        "\n",
        "weight_scale = 7*(1.0-beta) # this should give us some spikes to begin with\n",
        "\n",
        "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale/np.sqrt(nb_inputs))\n",
        "\n",
        "\n",
        "nb_bias2 = 2\n",
        "weight_scale = np.sqrt(2)\n",
        "\n",
        "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(w2, mean=-0.275419*weight_scale/np.sqrt(nb_inputs+nb_hidden+nb_bias2), std=weight_scale/np.sqrt(nb_hidden+nb_outputs+nb_bias2))\n",
        "\n",
        "bias2_time = torch.empty((nb_bias2),  device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.uniform_(bias2_time, a=0.0, b=1.0)\n",
        "bias2_weight = torch.empty((nb_bias2, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
        "torch.nn.init.normal_(bias2_weight, mean=7.83912*weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden), std=weight_scale/np.sqrt(nb_outputs+nb_bias2+nb_hidden))\n",
        "#torch.nn.init.uniform_(bias2_weight, a=1.0, b=1.0)\n",
        "\n",
        "print(\"init done\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HU7k8ngXMzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#[w1,w2, bias2_weight, bias2_time] = torch.load('params_surro_n_google.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGQFh2hCJZsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "nb_epochs = 20\n",
        "batch_size =50\n",
        "\n",
        "lr = 2e-3\n",
        "lr_t = 2e-3\n",
        "lr_pulse = 6e-2\n",
        "\n",
        "params = [w1]\n",
        "params_t = [w2, bias2_weight]\n",
        "params_pulse = [bias2_time]\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=lr, betas=(0.9,0.999))\n",
        "optimizer_t = torch.optim.Adam(params_t, lr=lr_t, betas=(0.9,0.999))\n",
        "optimizer_pulse = torch.optim.Adam(params_pulse, lr=lr_pulse, betas=(0.9,0.999))\n",
        "\n",
        "#error_t = spike_loss_temporary(time_step = 1, device = device)\n",
        "log_softmax_fn = nn.LogSoftmax(dim=1)\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "loss_hist = []\n",
        "training_accus = []\n",
        "test_accus = []\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_spks_list = np.zeros((20))\n",
        "for e in range(nb_epochs):\n",
        "    loss_list = []\n",
        "    local_loss = []\n",
        "    i=0\n",
        "    for x_local, y_local in dense_data_generator(training_addrs, training_labels, batch_size):\n",
        "        zeros = torch.zeros((x_local.shape[0], 200, x_local.shape[2])).to(device)\n",
        "        x_local = torch.cat((x_local, zeros),1)\n",
        "        #print(x_local.shape)\n",
        "        x_local = Variable(x_local, requires_grad=True)\n",
        "        output,n_spks = run_snn2(x_local)\n",
        "        loss_val = loss_fn(-output, y_local)#+reg_loss\n",
        "        n_spks_list[i]=n_spks/batch_size        \n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        optimizer_t.zero_grad()\n",
        "        optimizer_pulse.zero_grad()\n",
        "\n",
        "        loss_val.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer_t.step()\n",
        "        optimizer_pulse.step()\n",
        "\n",
        "        local_loss.append(loss_val.detach().item())\n",
        "        if i%5 == 0:\n",
        "          print('batch', i)\n",
        "        i+=1\n",
        "    x_local =[]\n",
        "    y_local =[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      mean_loss = np.mean(local_loss)\n",
        "      loss_hist.append(mean_loss)\n",
        "      training_accu = compute_classification_accuracy(training_addrs,training_labels)\n",
        "      \n",
        "      training_accus.append(training_accu)\n",
        "      test_accu = compute_classification_accuracy(test_addrs,test_labels)\n",
        "      test_accus.append(test_accu)\n",
        "    print(w1.requires_grad)\n",
        "    print('epoch', e, 'loss', mean_loss, 'training accuracy ', training_accu, 'test accuracy ', test_accu, 'n_spks', np.mean(n_spks_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPPBV810XCTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save([w1,w2, bias2_weight, bias2_time], 'params_surro_n_google.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}